#### BigMart Sales Prediction!

## Sales Prediction for Big Mart Outlets

# The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities.
# Also, certain attributes of each product and store have been defined. 
# The aim is to build a predictive model and predict the sales of each product at a particular outlet.
# Using this model, BigMart will try to understand the properties of products and outlets which play a key role in increasing sales.

#### Importing the necessary libraries

# Warnings
import warnings
warnings.filterwarnings("ignore")

# Data Handling & Visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Data Preprocessing
from sklearn.impute import KNNImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split

# Machine Learning Models
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor

# Hyperparameter Tuning & Evaluation
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Reading the train & test datasets
df_data = pd.read_csv('train_v9rqX0R.csv')
df_test = pd.read_csv('test_AbJTz2l.csv')

print(df_train.shape)
print(df_train.columns)

#### EDA

df_train.describe()

df_train.info()

# Item_Weight(numerical) & Outlet_Size(Categorical) columns have Null values 

print(df_train.dtypes)

null_percentage = (df_train.isnull().sum() / len(df_train)) * 100
null_percentage = null_percentage[null_percentage > 0]  
print(null_percentage)

# Null values in test data

null_percentage = (df_test.isnull().sum() / len(df_test)) * 100
null_percentage = null_percentage[null_percentage > 0]  
print(null_percentage)

plt.figure(figsize=(10, 6))
sns.heatmap(df_train.isnull(), cmap='viridis', cbar=False, yticklabels=False)
plt.title("Missing Values Heatmap")
plt.show()

imputer = KNNImputer(n_neighbors=4)
df_train[['Item_Weight']] = imputer.fit_transform(df_train[['Item_Weight']])

df_test[['Item_Weight']] = imputer.transform(df_test[['Item_Weight']])

df_train.Outlet_Size.value_counts()

# Creating new label for Null values in Outlet Size column
df_train['Outlet_Size'] = df_train['Outlet_Size'].fillna("Unknown")

df_test['Outlet_Size'] = df_test['Outlet_Size'].fillna("Unknown")

df_train.isnull().sum()

df_test.isnull().sum()

plt.figure(figsize=(8, 4))
sns.countplot(y=df_train['Outlet_Size'], order=df_train['Outlet_Size'].value_counts().index)
plt.title(f"Category distribution of Outlet_Size")
plt.show()

categorical_cols = df_train.select_dtypes(include=['object', 'category']).columns
categorical_cols

categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']

df_train[categorical_cols].nunique()

for col in categorical_cols:
    print(f'\nFor {col} column : ')
    print(df_train[col].value_counts())

# Identify categorical and numerical columns
categorical_cols = df_train.select_dtypes(include=['object', 'category']).columns
categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
numerical_cols = df_train.select_dtypes(include=['int64', 'float64']).columns

# Loop through each categorical column and plot a count plot
for col in categorical_cols:
    plt.figure(figsize=(8, 4))  # Set consistent figure size for each plot
    ax = sns.countplot(y=df_train[col], order=df_train[col].value_counts().index)

    plt.title(f"Category Distribution of {col}")
    plt.xlabel("Count")
    plt.ylabel(col)

    # Add percentage labels
    total = len(df_train[col])
    for p in ax.patches:
        width = p.get_width()
        percentage = f"{(width / total) * 100:.1f}%"  # Calculate percentage
        ax.text(width + 1, p.get_y() + p.get_height() / 2, percentage, ha="left", va="center", fontsize=10)

    plt.show()

colors = sns.color_palette("Set1", len(numerical_cols))

plt.figure(figsize=(12, len(numerical_cols) * 4))

for i, (col, color) in enumerate(zip(numerical_cols, colors), 1):
    plt.subplot(len(numerical_cols), 1, i)
    df_train[col].plot(kind="hist", bins=30, color=color, edgecolor="black", alpha=0.7)  # Darker, more visible

    plt.title(f"Distribution of {col}", fontsize=14, fontweight='bold', color=color)
    plt.xlabel(col, fontsize=12)
    plt.ylabel("Frequency", fontsize=12)

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, len(numerical_cols) * 4))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(len(numerical_cols), 1, i)  # Subplots for each column
    sns.boxplot(x=df_train[col], color='#99FF99')  # Box plot with dark blue color
    
    plt.title(f"Box Plot of {col}")
    plt.xlabel(col)

plt.tight_layout()
plt.show()

# Features to compare with Item_Outlet_Sales
features = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']

# Set figure size
plt.figure(figsize=(12, len(features) * 4))

# Loop through each feature and plot
for i, col in enumerate(features, 1):
    plt.subplot(len(features), 1, i)
    plt.scatter(df_train[col], df_train['Item_Outlet_Sales'], alpha=0.5, color="#FF9933")  # Scatter plot
    plt.title(f"{col} vs. Item_Outlet_Sales")
    plt.xlabel(col)
    plt.ylabel("Item_Outlet_Sales")

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

# Trying to see if any specific products were introduced in the early years
df_train['Outlet_Establishment_Year'].nunique()
df_train['Outlet_Establishment_Year'].unique()
np.sort(df_train['Outlet_Establishment_Year'].unique())

# Filter dataset for outlets established in 1985 & 1987
filtered_items = df_train[df_train['Outlet_Establishment_Year'].isin([1985, 1987])]

# Get unique Item Identifiers
unique_items_85_87 = filtered_items['Item_Identifier'].unique()

# Display the results
print("Unique Item Identifiers sold in 1985 & 1987 stores:", unique_items_85_87)

filtered_items['Item_Identifier'].nunique() # No patterns could be found from the list


# Define bins and labels
bins = [1980, 1990, 2000, 2010]  # Year ranges
labels = ['Old (1980-1990)', 'Medium (1991-2000)', 'New (2001-2010)']
# Create new column
df_train['Outlet_Age_Category'] = pd.cut(df_train['Outlet_Establishment_Year'], bins=bins, labels=labels)

# Check distribution
df_train['Outlet_Age_Category'].value_counts()
df_test['Outlet_Age_Category'] = pd.cut(df_test['Outlet_Establishment_Year'], bins=bins, labels=labels)

scaler = MinMaxScaler()
df_train['Item_Weight_MinMax'] = scaler.fit_transform(df_train[['Item_Weight']])
df_test['Item_Weight_MinMax'] = scaler.transform(df_test[['Item_Weight']])

df_train.drop(columns=['Item_Weight', 'Outlet_Establishment_Year'], inplace=True)
df_test.drop(columns=['Item_Weight', 'Outlet_Establishment_Year'], inplace=True)

for col in df_train.select_dtypes(include=['object']).columns:
    df_train[col] = df_train[col].astype('category')

for col in df_test.select_dtypes(include=['object']).columns:
    df_test[col] = df_test[col].astype('category')

#### Feature Engineering

# converting 'low fat' & 'LF' to 'Low Fat' label as they represent the same type
# similarly changing 'reg' to 'Regular' type

df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace({
    'low fat': 'Low Fat', 
    'LF': 'Low Fat', 
    'reg': 'Regular'
})

df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].replace({
    'low fat': 'Low Fat', 
    'LF': 'Low Fat', 
    'reg': 'Regular'
})

plt.figure(figsize=(8, 4))  # Set consistent figure size for each plot
ax = sns.countplot(y=df_train['Item_Fat_Content'], order=df_train['Item_Fat_Content'].value_counts().index)
plt.title(f"Category Distribution of Item_Fat_Content")
plt.xlabel("Count")
plt.ylabel(col)

# Add percentage labels
total = len(df_train['Item_Fat_Content'])
for p in ax.patches:
    width = p.get_width()
    percentage = f"{(width / total) * 100:.1f}%"  # Calculate percentage
    ax.text(width + 1, p.get_y() + p.get_height() / 2, percentage, ha="left", va="center", fontsize=10)
plt.show()

#### Grouping Less Frequent Categories
df_train['Outlet_Type_Grouped'] = df_train['Outlet_Type'].replace({
    'Grocery Store': 'Other', 
    'Supermarket Type3': 'Other', 
    'Supermarket Type2': 'Other'
})

# Grouping Less Frequent Categories

df_test['Outlet_Type_Grouped'] = df_test['Outlet_Type'].replace({
    'Grocery Store': 'Other', 
    'Supermarket Type3': 'Other', 
    'Supermarket Type2': 'Other'
})

plt.figure(figsize=(8, 4))  # Set consistent figure size for each plot
ax = sns.countplot(y=df_train['Outlet_Type_Grouped'], order=df_train['Outlet_Type_Grouped'].value_counts().index)

plt.title(f"Category Distribution of Outlet_Type_Grouped")
plt.xlabel("Count")
plt.ylabel(col)

# Add percentage labels
total = len(df_train['Outlet_Type_Grouped'])
for p in ax.patches:
    width = p.get_width()
    percentage = f"{(width / total) * 100:.1f}%"  # Calculate percentage
    ax.text(width + 1, p.get_y() + p.get_height() / 2, percentage, ha="left", va="center", fontsize=10)
plt.show()

# Dropping Outlet_Type column
df_train.drop(columns=['Outlet_Type'], inplace=True)
df_test.drop(columns=['Outlet_Type'], inplace=True)

# Define categorical columns
label_encode_cols = ['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Age_Category']
onehot_encode_cols = ['Item_Identifier', 'Outlet_Identifier', 'Item_Type', 'Outlet_Type_Grouped']
numerical_cols = ['Item_Visibility', 'Item_Weight_MinMax', 'Item_MRP']

# Separate features and target variable
X = df_train[label_encode_cols + onehot_encode_cols + numerical_cols]
y = df_train['Item_Outlet_Sales']

# Compute correlation of numerical columns with the target variable
correlation_matrix = X[numerical_cols].join(y).corr()
# Extract correlation values with sales
corr_with_sales = correlation_matrix[['Item_Outlet_Sales']].sort_values(by='Item_Outlet_Sales', ascending=False)
print(corr_with_sales)

plt.figure(figsize=(8, 5))
sns.heatmap(corr_with_sales, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f", cbar=True)
plt.title("Correlation of Numerical Features with Item_Outlet_Sales")
plt.show()

### Dropping columns with very less correlation with the Sales column
# Item_Visibility -> 0 correlation
# Item_Weight_MinMax -> -0.13 correlation
X.drop(columns=['Item_Visibility', 'Item_Weight_MinMax'], inplace=True)
df_test.drop(columns=['Item_Visibility', 'Item_Weight_MinMax'], inplace=True)

for col in X.select_dtypes(include=['object']).columns:
    X[col] = X[col].astype('category')
for col in df_test.select_dtypes(include=['object']).columns:
    df_test[col] = df_test[col].astype('category')

df_test = df_test[X.columns]
encoder_dict = {}  # Store fitted encoders for later use
cat_cols = ['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type',
            'Outlet_Age_Category', 'Item_Identifier', 'Outlet_Identifier',
            'Item_Type', 'Outlet_Type_Grouped']

# Fit encoders on X (train data) and store them
for col in cat_cols:
    encoder = LabelEncoder()
    X[col] = encoder.fit_transform(X[col])  # Fit & transform training data
    encoder_dict[col] = encoder  # Save encoder for later use

# Apply the same encoders to df_test
for col in cat_cols:
    df_test[col] = encoder_dict[col].transform(df_test[col])  # Transform df_test

scaler = StandardScaler()
X['Item_MRP'] = scaler.fit_transform(X[['Item_MRP']])
df_test['Item_MRP'] = scaler.transform(df_test[['Item_MRP']])



### Train Test Split -> using 10% of training data as validation data
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.1, random_state = 26)

print("Shape of X Train: ",X_train.shape)
print("Shape of X Test: ",X_test.shape)
print("Shape of y Train: ",y_train.shape)
print("Shape of y Test: ",y_test.shape)

print('Shape of unseen data', df_test.shape)

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(max_depth=5, random_state=26),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=26),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=26)
}
rmse_scores = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    
    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # RMSE Calculation
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    
    rmse_scores[name] = {"Train RMSE": train_rmse, "Test RMSE": test_rmse}
    
    print(f"****** {name} ******:")
    print(f"   Train RMSE: {train_rmse:.4f}")
    print(f"   Test RMSE: {test_rmse:.4f}\n")

# Define the hyperparameter grid for Random Forest
rf_param_grid = {
    'n_estimators': [100, 300],
    'max_depth': [10, 20, 15],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 4]
}

# Initialize RandomForest model
rf = RandomForestRegressor(random_state=26)

# GridSearchCV for Random Forest
rf_grid_search = GridSearchCV(rf, rf_param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=1)
rf_grid_search.fit(X_train, y_train)

# Best Parameters
print("Best Random Forest Parameters:", rf_grid_search.best_params_)

# Train with best parameters
rf_best = rf_grid_search.best_estimator_

# Predict on train and test data
rf_train_preds = rf_best.predict(X_train)
rf_test_preds = rf_best.predict(X_test)

# Compute RMSE
rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_preds))
rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_preds))

print(f"Random Forest Train RMSE: {rf_train_rmse:.4f}")
print(f"Random Forest Test RMSE: {rf_test_rmse:.4f}")

# Define the hyperparameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 300],
    'max_depth': [3, 6],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize XGBoost model
xgb = XGBRegressor(random_state=26, objective='reg:squarederror')

# GridSearchCV for XGBoost
xgb_grid_search = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=-1)
xgb_grid_search.fit(X_train, y_train)

# Best Parameters
print("Best XGBoost Parameters:", xgb_grid_search.best_params_)

# Train with best parameters
xgb_best = xgb_grid_search.best_estimator_

# Predict on train and test data
xgb_train_preds = xgb_best.predict(X_train)
xgb_test_preds = xgb_best.predict(X_test)

# Compute RMSE
xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_preds))
xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_preds))

print(f"XGBoost Train RMSE: {xgb_train_rmse:.4f}")
print(f"XGBoost Test RMSE: {xgb_test_rmse:.4f}")

## Trail 2 -> using 20% of training data as validation data
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.20, random_state = 26)

print("Shape of X Train: ",X_train.shape)
print("Shape of X Test: ",X_test.shape)
print("Shape of y Train: ",y_train.shape)
print("Shape of y Test: ",y_test.shape)

print('Shape of unseen data', df_test.shape)

# Define the hyperparameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 300],
    'max_depth': [3, 6],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize XGBoost model
xgb = XGBRegressor(random_state=26, objective='reg:squarederror')

# GridSearchCV for XGBoost
xgb_grid_search = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=-1)
xgb_grid_search.fit(X_train, y_train)

# Train with best parameters
xgb_best = xgb_grid_search.best_estimator_

# Predict on train and test data
xgb_train_preds = xgb_best.predict(X_train)
xgb_test_preds = xgb_best.predict(X_test)

# Compute RMSE
xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_preds))
xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_preds))

print(f"XGBoost Train RMSE: {xgb_train_rmse:.4f}")
print(f"XGBoost Test RMSE: {xgb_test_rmse:.4f}")

df_sub = pd.read_csv('test_AbJTz2l.csv')
xgb_sub_preds = xgb_best.predict(df_test)

submission_predictions = pd.Series(xgb_sub_preds)
submission_csv = df_sub[['Item_Identifier', 'Outlet_Identifier']]
submission_csv['Item_Outlet_Sales'] = submission_predictions
submission_csv['Item_Outlet_Sales'] = np.maximum(submission_csv['Item_Outlet_Sales'], 0)

submission_csv.to_csv('BigMart_Submission_csv_03_03_2025.csv', index = False)

############################################ End of Code ############################################ 

